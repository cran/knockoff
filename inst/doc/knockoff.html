<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="date" content="2017-10-16" />

<title>Controlled variable Selection with Model-X Knockoffs</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">Controlled variable Selection with Model-X Knockoffs</h1>
<h4 class="date"><em>2017-10-16</em></h4>



<p>This vignette illustrates the basic usage of the <code>knockoff</code> package with Model-X knockoffs. In this scenario we assume that the distribution of the predictors is known (or that it can be well approximated), but we make no assumptions on the conditional distribution of the response. For simplicity, we will use synthetic data constructed from a linear model such that the response only depends on a small fraction of the variables.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1234</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Problem parameters</span>
n =<span class="st"> </span><span class="dv">1000</span>          <span class="co"># number of observations</span>
p =<span class="st"> </span><span class="dv">1000</span>          <span class="co"># number of variables</span>
k =<span class="st"> </span><span class="dv">60</span>            <span class="co"># number of variables with nonzero coefficients</span>
amplitude =<span class="st"> </span><span class="fl">4.5</span>   <span class="co"># signal amplitude (for noise level = 1)</span>

<span class="co"># Generate the variables from a multivariate normal distribution</span>
mu =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,p)
rho =<span class="st"> </span><span class="fl">0.25</span>
Sigma =<span class="st"> </span><span class="kw">toeplitz</span>(rho^(<span class="dv">0</span>:(p<span class="dv">-1</span>)))
X =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n*p),n) %*%<span class="st"> </span><span class="kw">chol</span>(Sigma)

<span class="co"># Generate the response from a linear model</span>
nonzero =<span class="st"> </span><span class="kw">sample</span>(p, k)
beta =<span class="st"> </span>amplitude *<span class="st"> </span>(<span class="dv">1</span>:p %in%<span class="st"> </span>nonzero) /<span class="st"> </span><span class="kw">sqrt</span>(n)
y.sample =<span class="st"> </span>function(X) X %*%<span class="st"> </span>beta +<span class="st"> </span><span class="kw">rnorm</span>(n)
y =<span class="st"> </span><span class="kw">y.sample</span>(X)</code></pre></div>
<div id="first-examples" class="section level2">
<h2>First examples</h2>
<p>To begin, we call <code>knockoff.filter</code> with all the default settings.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(knockoff)
result =<span class="st"> </span><span class="kw">knockoff.filter</span>(X, y)</code></pre></div>
<p>We can display the results with</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(result)</code></pre></div>
<pre><code>## Call:
## knockoff.filter(X = X, y = y)
## 
## Selected variables:
##  [1]   3   9  40  44  46  61  67  78  85 108 148 153 172 173 177 210 223
## [18] 238 248 281 295 301 302 317 319 326 334 343 360 364 378 384 389 421
## [35] 426 428 451 494 506 510 528 534 557 559 595 596 617 668 676 682 708
## [52] 770 775 787 836 844 875 893 906 913 931 937 953 959</code></pre>
<p>The default value for the target false discovery rate is 0.1. In this experiment the false discovery proportion is</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fdp =<span class="st"> </span>function(selected) <span class="kw">sum</span>(beta[selected] ==<span class="st"> </span><span class="dv">0</span>) /<span class="st"> </span><span class="kw">max</span>(<span class="dv">1</span>, <span class="kw">length</span>(selected))
<span class="kw">fdp</span>(result$selected)</code></pre></div>
<pre><code>## [1] 0.171875</code></pre>
<p>By default, the knockoff filter creates model-X second-order Gaussian knockoffs. This construction estimates from the data the mean <span class="math inline">\(\mu\)</span> and the covariance <span class="math inline">\(\Sigma\)</span> of the rows of <span class="math inline">\(X\)</span>, instead of using the true parameters (<span class="math inline">\(\mu, \Sigma\)</span>) from which the variables were sampled.</p>
<p>The knockoff package also includes other knockoff construction methods, all of which have names prefixed with<code>knockoff.create</code>. In the next snippet, we generate knockoffs using the true model parameters.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">gaussian_knockoffs =<span class="st"> </span>function(X) <span class="kw">create.gaussian</span>(X, mu, Sigma)
result =<span class="st"> </span><span class="kw">knockoff.filter</span>(X, y, <span class="dt">knockoffs=</span>gaussian_knockoffs)
<span class="kw">print</span>(result)</code></pre></div>
<pre><code>## Call:
## knockoff.filter(X = X, y = y, knockoffs = gaussian_knockoffs)
## 
## Selected variables:
##  [1]   3   9  40  44  46  61  85 108 148 153 172 173 177 210 223 238 248
## [18] 295 301 319 326 334 343 360 364 378 384 389 421 426 428 451 506 557
## [35] 559 595 668 708 770 775 844 893 906 913 931 937 953 959</code></pre>
<p>Now the false discovery proportion is</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">fdp</span>(result$selected)</code></pre></div>
<pre><code>## [1] 0.0625</code></pre>
<p>By default, the knockoff filter uses a test statistic based on the lasso. Specifically, it uses the statistic <code>stat.glmnet_coefdiff</code>, which computes <span class="math display">\[
W_j = |Z_j| - |\tilde{Z}_j|
\]</span> where <span class="math inline">\(Z_j\)</span> and <span class="math inline">\(\tilde{Z}_j\)</span> are the lasso coefficient estimates for the jth variable and its knockoff, respectively. The value of the regularization parameter <span class="math inline">\(\lambda\)</span> is selected by cross-validation and computed with <code>glmnet</code>.</p>
<p>Several other built-in statistics are available, all of which have names prefixed with <code>stat</code>. For example, we can use statistics based on random forests. In addition to choosing different statistics, we can also vary the target FDR level (e.g. we now increase it to 0.2).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">result =<span class="st"> </span><span class="kw">knockoff.filter</span>(X, y, <span class="dt">knockoffs =</span> gaussian_knockoffs, <span class="dt">statistic =</span> stat.random_forest, <span class="dt">fdr=</span><span class="fl">0.2</span>)
<span class="kw">print</span>(result)</code></pre></div>
<pre><code>## Call:
## knockoff.filter(X = X, y = y, knockoffs = gaussian_knockoffs, 
##     statistic = stat.random_forest, fdr = 0.2)
## 
## Selected variables:
##  [1]   9  85 108 148 172 173 210 223 238 248 301 334 343 384 426 428 557
## [18] 595 668 708 770 785 906 931 953</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">fdp</span>(result$selected)</code></pre></div>
<pre><code>## [1] 0.08</code></pre>
</div>
<div id="user-defined-test-statistics" class="section level2">
<h2>User-defined test statistics</h2>
<p>In addition to using the predefined test statistics, it is also possible to use your own custom test statistics. To illustrate this functionality, we implement one of the simplest test statistics from the original knockoff filter paper, namely <span class="math display">\[
W_j = \left|X_j^\top \cdot y\right| - \left|\tilde{X}_j^\top \cdot y\right|.
\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">my_knockoff_stat =<span class="st"> </span>function(X, X_k, y) {
  <span class="kw">abs</span>(<span class="kw">t</span>(X) %*%<span class="st"> </span>y) -<span class="st"> </span><span class="kw">abs</span>(<span class="kw">t</span>(X_k) %*%<span class="st"> </span>y)
}
result =<span class="st"> </span><span class="kw">knockoff.filter</span>(X, y, <span class="dt">knockoffs =</span> gaussian_knockoffs, <span class="dt">statistic =</span> my_knockoff_stat)
<span class="kw">print</span>(result)</code></pre></div>
<pre><code>## Call:
## knockoff.filter(X = X, y = y, knockoffs = gaussian_knockoffs, 
##     statistic = my_knockoff_stat)
## 
## Selected variables:
##  [1] 108 148 173 223 238 248 274 301 421 426 668 708 906 931 937 953 959</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">fdp</span>(result$selected)</code></pre></div>
<pre><code>## [1] 0.1176471</code></pre>
<p>As another example, we show how to customize the grid of <span class="math inline">\(\lambda\)</span>’s used to compute the lasso path in the default test statistic.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">my_lasso_stat =<span class="st"> </span>function(...) <span class="kw">stat.glmnet_coefdiff</span>(..., <span class="dt">nlambda=</span><span class="dv">100</span>)
result =<span class="st"> </span><span class="kw">knockoff.filter</span>(X, y, <span class="dt">knockoffs =</span> gaussian_knockoffs, <span class="dt">statistic =</span> my_lasso_stat)
<span class="kw">print</span>(result)</code></pre></div>
<pre><code>## Call:
## knockoff.filter(X = X, y = y, knockoffs = gaussian_knockoffs, 
##     statistic = my_lasso_stat)
## 
## Selected variables:
##  [1]   9  40  44  46  61  67  78  85 104 108 124 141 148 153 172 173 177
## [18] 210 223 238 248 279 281 295 301 302 317 319 326 334 343 360 364 378
## [35] 384 389 421 426 428 451 494 506 510 528 534 538 557 559 595 596 617
## [52] 651 668 676 682 702 708 718 770 775 787 836 844 875 893 906 913 931
## [69] 937 953 959</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">fdp</span>(result$selected)</code></pre></div>
<pre><code>## [1] 0.2394366</code></pre>
<p>The <code>nlambda</code> parameter is passed by <code>stat.glmnet_coefdiff</code> to the <code>glmnet</code>, which is used to compute the lasso path. For more information about this and other parameters, see the documentation for <code>stat.glmnet_coefdiff</code> or <code>glmnet.glmnet</code>.</p>
</div>
<div id="user-defined-knockoff-generation-functions" class="section level2">
<h2>User-defined knockoff generation functions</h2>
<p>In addition to using the predefined procedures for construction knockoff variables, it is also possible to create your own knockoffs. To illustrate this functionality, we implement a simple wrapper for the construction of second-order Model-X knockoffs.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">create_knockoffs =<span class="st"> </span>function(X) {
  <span class="kw">create.second_order</span>(X, <span class="dt">shrink=</span>T)
}
result =<span class="st"> </span><span class="kw">knockoff.filter</span>(X, y, <span class="dt">knockoffs=</span>create_knockoffs)
<span class="kw">print</span>(result)</code></pre></div>
<pre><code>## Call:
## knockoff.filter(X = X, y = y, knockoffs = create_knockoffs)
## 
## Selected variables:
##  [1]   9  40  61  85 108 148 153 172 173 177 210 223 248 295 301 319 326
## [18] 334 343 360 364 378 384 389 421 426 428 451 506 559 595 668 708 770
## [35] 844 893 906 913 931 937 953 959</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">fdp</span>(result$selected)</code></pre></div>
<pre><code>## [1] 0</code></pre>
</div>
<div id="approximate-vs-full-sdp-knockoffs" class="section level2">
<h2>Approximate vs Full SDP knockoffs</h2>
<p>The knockoff package supports two main styles of knockoff variables, <em>semidefinite programming</em> (SDP) knockoffs (the default) and <em>equi-correlated</em> knockoffs. Though more computationally expensive, the SDP knockoffs are statistically superior by having higher power. To create SDP knockoffs, this package relies on the R library [Rdsdp][Rdsdp] to efficiently solve the semidefinite program. In high-dimensional settings, this program becomes computationally intractable. A solution is then offered by approximate SDP (ASDP) knockoffs, which address this issue by solving a simpler relaxed problem based on a block-diagonal approximation of the covariance matrix. By default, the knockoff filter uses SDP knockoffs if <span class="math inline">\(p&lt;500\)</span> and ASDP knockoffs otherwise.</p>
<p>In this example we generate second-order Gaussian knockoffs using the estimated model parameters and the full SDP construction. Then, we run the knockoff filter as usual.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">gaussian_knockoffs =<span class="st"> </span>function(X) <span class="kw">create.second_order</span>(X, <span class="dt">method=</span><span class="st">'sdp'</span>, <span class="dt">shrink=</span>T)
result =<span class="st"> </span><span class="kw">knockoff.filter</span>(X, y, <span class="dt">knockoffs =</span> gaussian_knockoffs)
<span class="kw">print</span>(result)</code></pre></div>
<pre><code>## Call:
## knockoff.filter(X = X, y = y, knockoffs = gaussian_knockoffs)
## 
## Selected variables:
##  [1]   9  40  44  46  61  78  85 108 141 146 148 153 172 173 177 210 238
## [18] 248 274 295 301 302 319 326 334 343 360 364 378 384 389 421 426 428
## [35] 451 494 506 510 528 559 595 617 651 668 676 682 702 708 718 770 775
## [52] 836 844 875 893 906 913 931 937 953 959</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">fdp</span>(result$selected)</code></pre></div>
<pre><code>## [1] 0.1967213</code></pre>
</div>
<div id="equi-correlated-knockoffs" class="section level2">
<h2>Equi-correlated knockoffs</h2>
<p>Equicorrelated knockoffs offer a computationally cheaper alternative to SDP knockoffs, at the cost of lower statistical power. In this example we generate second-order Gaussian knockoffs using the estimated model parameters and the equicorrelated construction. Then we run the knockoff filter.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">gaussian_knockoffs =<span class="st"> </span>function(X) <span class="kw">create.second_order</span>(X, <span class="dt">method=</span><span class="st">'equi'</span>, <span class="dt">shrink=</span>T)
result =<span class="st"> </span><span class="kw">knockoff.filter</span>(X, y, <span class="dt">knockoffs =</span> gaussian_knockoffs)
<span class="kw">print</span>(result)</code></pre></div>
<pre><code>## Call:
## knockoff.filter(X = X, y = y, knockoffs = gaussian_knockoffs)
## 
## Selected variables:
##  [1]   3   9  40  46  61  78  85 108 148 153 172 173 177 210 223 238 248
## [18] 281 295 301 326 334 343 360 364 378 384 389 421 426 428 451 494 506
## [35] 557 559 595 651 668 682 702 708 770 775 787 844 893 906 913 931 937
## [52] 953 959</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">fdp</span>(result$selected)</code></pre></div>
<pre><code>## [1] 0.0754717</code></pre>
</div>
<div id="see-also" class="section level2">
<h2>See also</h2>
<p>If you want to look inside the knockoff filter, see the <a href="advanced.html">advanced vignette</a>. If you want to see how to use knockoffs for Fixed-X variables, see the <a href="fixed.html">Fixed-X vignette</a>.</p>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
